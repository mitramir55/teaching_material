{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "read_file = pd.read_csv (r'C:\\Users\\mitra\\Downloads\\Global_Citizen_predictions (1).csv', encoding='unicode_escape', low_memory = False)\n",
    "#read_file.to_excel (r'file', index = None, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>sent_indices</th>\n",
       "      <th>doc_indices</th>\n",
       "      <th>index</th>\n",
       "      <th>Author</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Links</th>\n",
       "      <th>Full Text</th>\n",
       "      <th>Title</th>\n",
       "      <th>Publication</th>\n",
       "      <th>...</th>\n",
       "      <th>Publication Date</th>\n",
       "      <th>Section</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Place of publication</th>\n",
       "      <th>Publication subject</th>\n",
       "      <th>Source type</th>\n",
       "      <th>Document type</th>\n",
       "      <th>Publication Location</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>Global_Citizen_predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gunnar Hkmark is the president of the Stockho...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Hkmark, Gunnar; Oksanen, Patrik</td>\n",
       "      <td>None available.</td>\n",
       "      <td>http://resolver.library.ualberta.ca/resolver?u...</td>\n",
       "      <td>Gunnar Hkmark is the president of the Stockho...</td>\n",
       "      <td>How Sweden can collaborate with Canada: Ê The ...</td>\n",
       "      <td>The Globe and Mail</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-07-13</td>\n",
       "      <td>News</td>\n",
       "      <td>The Globe and Mail</td>\n",
       "      <td>Toronto, Ont.</td>\n",
       "      <td>General Interest Periodicals--Canada</td>\n",
       "      <td>Newspaper</td>\n",
       "      <td>Opinions, Commentary</td>\n",
       "      <td>Toronto, Ont.</td>\n",
       "      <td>gunnar hkmark president stockholm free world f...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Patrik Oksanen is a senior fellow of Stockholm...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Hkmark, Gunnar; Oksanen, Patrik</td>\n",
       "      <td>None available.</td>\n",
       "      <td>http://resolver.library.ualberta.ca/resolver?u...</td>\n",
       "      <td>Gunnar Hkmark is the president of the Stockho...</td>\n",
       "      <td>How Sweden can collaborate with Canada: Ê The ...</td>\n",
       "      <td>The Globe and Mail</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-07-13</td>\n",
       "      <td>News</td>\n",
       "      <td>The Globe and Mail</td>\n",
       "      <td>Toronto, Ont.</td>\n",
       "      <td>General Interest Periodicals--Canada</td>\n",
       "      <td>Newspaper</td>\n",
       "      <td>Opinions, Commentary</td>\n",
       "      <td>Toronto, Ont.</td>\n",
       "      <td>patrik oksanen senior fellow stockholm free wo...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sweden and Canada share many values and intere...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Hkmark, Gunnar; Oksanen, Patrik</td>\n",
       "      <td>None available.</td>\n",
       "      <td>http://resolver.library.ualberta.ca/resolver?u...</td>\n",
       "      <td>Gunnar Hkmark is the president of the Stockho...</td>\n",
       "      <td>How Sweden can collaborate with Canada: Ê The ...</td>\n",
       "      <td>The Globe and Mail</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-07-13</td>\n",
       "      <td>News</td>\n",
       "      <td>The Globe and Mail</td>\n",
       "      <td>Toronto, Ont.</td>\n",
       "      <td>General Interest Periodicals--Canada</td>\n",
       "      <td>Newspaper</td>\n",
       "      <td>Opinions, Commentary</td>\n",
       "      <td>Toronto, Ont.</td>\n",
       "      <td>sweden canada share many values interests</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The two Arctic countries have mutual concerns ...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Hkmark, Gunnar; Oksanen, Patrik</td>\n",
       "      <td>None available.</td>\n",
       "      <td>http://resolver.library.ualberta.ca/resolver?u...</td>\n",
       "      <td>Gunnar Hkmark is the president of the Stockho...</td>\n",
       "      <td>How Sweden can collaborate with Canada: Ê The ...</td>\n",
       "      <td>The Globe and Mail</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-07-13</td>\n",
       "      <td>News</td>\n",
       "      <td>The Globe and Mail</td>\n",
       "      <td>Toronto, Ont.</td>\n",
       "      <td>General Interest Periodicals--Canada</td>\n",
       "      <td>Newspaper</td>\n",
       "      <td>Opinions, Commentary</td>\n",
       "      <td>Toronto, Ont.</td>\n",
       "      <td>two arctic countries mutual concerns imperiali...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Both Sweden and Canada have also suffered from...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Hkmark, Gunnar; Oksanen, Patrik</td>\n",
       "      <td>None available.</td>\n",
       "      <td>http://resolver.library.ualberta.ca/resolver?u...</td>\n",
       "      <td>Gunnar Hkmark is the president of the Stockho...</td>\n",
       "      <td>How Sweden can collaborate with Canada: Ê The ...</td>\n",
       "      <td>The Globe and Mail</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-07-13</td>\n",
       "      <td>News</td>\n",
       "      <td>The Globe and Mail</td>\n",
       "      <td>Toronto, Ont.</td>\n",
       "      <td>General Interest Periodicals--Canada</td>\n",
       "      <td>Newspaper</td>\n",
       "      <td>Opinions, Commentary</td>\n",
       "      <td>Toronto, Ont.</td>\n",
       "      <td>sweden canada also suffered troubling forest f...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51089</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>1986</td>\n",
       "      <td>1988</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51090</th>\n",
       "      <td>\"\\n\"Like any modern army we find the more we i...</td>\n",
       "      <td>11</td>\n",
       "      <td>1986</td>\n",
       "      <td>1988</td>\n",
       "      <td>Graney, Juris</td>\n",
       "      <td>None available.</td>\n",
       "      <td>http://resolver.library.ualberta.ca/resolver?u...</td>\n",
       "      <td>Canada has deployed fresh troops from Edmonton...</td>\n",
       "      <td>Canadians sent on training mission; Edmonton-b...</td>\n",
       "      <td>The Simcoe Reformer</td>\n",
       "      <td>...</td>\n",
       "      <td>2016-08-05</td>\n",
       "      <td>National News</td>\n",
       "      <td>Postmedia Network Inc.</td>\n",
       "      <td>Calgary</td>\n",
       "      <td>General Interest Periodicals--Canada</td>\n",
       "      <td>Newspaper</td>\n",
       "      <td>News</td>\n",
       "      <td>Calgary</td>\n",
       "      <td>like modern army find interoperate partners al...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51091</th>\n",
       "      <td>Not a Sentence</td>\n",
       "      <td>12</td>\n",
       "      <td>1986</td>\n",
       "      <td>1988</td>\n",
       "      <td>Graney, Juris</td>\n",
       "      <td>None available.</td>\n",
       "      <td>http://resolver.library.ualberta.ca/resolver?u...</td>\n",
       "      <td>Canada has deployed fresh troops from Edmonton...</td>\n",
       "      <td>Canadians sent on training mission; Edmonton-b...</td>\n",
       "      <td>The Simcoe Reformer</td>\n",
       "      <td>...</td>\n",
       "      <td>2016-08-05</td>\n",
       "      <td>National News</td>\n",
       "      <td>Postmedia Network Inc.</td>\n",
       "      <td>Calgary</td>\n",
       "      <td>General Interest Periodicals--Canada</td>\n",
       "      <td>Newspaper</td>\n",
       "      <td>News</td>\n",
       "      <td>Calgary</td>\n",
       "      <td>sentence</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51092</th>\n",
       "      <td>twitter.</td>\n",
       "      <td>13</td>\n",
       "      <td>1986</td>\n",
       "      <td>1988</td>\n",
       "      <td>Graney, Juris</td>\n",
       "      <td>None available.</td>\n",
       "      <td>http://resolver.library.ualberta.ca/resolver?u...</td>\n",
       "      <td>Canada has deployed fresh troops from Edmonton...</td>\n",
       "      <td>Canadians sent on training mission; Edmonton-b...</td>\n",
       "      <td>The Simcoe Reformer</td>\n",
       "      <td>...</td>\n",
       "      <td>2016-08-05</td>\n",
       "      <td>National News</td>\n",
       "      <td>Postmedia Network Inc.</td>\n",
       "      <td>Calgary</td>\n",
       "      <td>General Interest Periodicals--Canada</td>\n",
       "      <td>Newspaper</td>\n",
       "      <td>News</td>\n",
       "      <td>Calgary</td>\n",
       "      <td>twitter</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51093</th>\n",
       "      <td>com/jurisgraney</td>\n",
       "      <td>14</td>\n",
       "      <td>1986</td>\n",
       "      <td>1988</td>\n",
       "      <td>Graney, Juris</td>\n",
       "      <td>None available.</td>\n",
       "      <td>http://resolver.library.ualberta.ca/resolver?u...</td>\n",
       "      <td>Canada has deployed fresh troops from Edmonton...</td>\n",
       "      <td>Canadians sent on training mission; Edmonton-b...</td>\n",
       "      <td>The Simcoe Reformer</td>\n",
       "      <td>...</td>\n",
       "      <td>2016-08-05</td>\n",
       "      <td>National News</td>\n",
       "      <td>Postmedia Network Inc.</td>\n",
       "      <td>Calgary</td>\n",
       "      <td>General Interest Periodicals--Canada</td>\n",
       "      <td>Newspaper</td>\n",
       "      <td>News</td>\n",
       "      <td>Calgary</td>\n",
       "      <td>comjurisgraney</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51094 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentences  sent_indices  \\\n",
       "0      Gunnar Hkmark is the president of the Stockho...             1   \n",
       "1      Patrik Oksanen is a senior fellow of Stockholm...             2   \n",
       "2      Sweden and Canada share many values and intere...             3   \n",
       "3      The two Arctic countries have mutual concerns ...             4   \n",
       "4      Both Sweden and Canada have also suffered from...             5   \n",
       "...                                                  ...           ...   \n",
       "51089                                                NaN            10   \n",
       "51090  \"\\n\"Like any modern army we find the more we i...            11   \n",
       "51091                                     Not a Sentence            12   \n",
       "51092                                           twitter.            13   \n",
       "51093                                    com/jurisgraney            14   \n",
       "\n",
       "       doc_indices  index                            Author         Abstract  \\\n",
       "0                1      0  Hkmark, Gunnar; Oksanen, Patrik  None available.   \n",
       "1                1      0  Hkmark, Gunnar; Oksanen, Patrik  None available.   \n",
       "2                1      0  Hkmark, Gunnar; Oksanen, Patrik  None available.   \n",
       "3                1      0  Hkmark, Gunnar; Oksanen, Patrik  None available.   \n",
       "4                1      0  Hkmark, Gunnar; Oksanen, Patrik  None available.   \n",
       "...            ...    ...                               ...              ...   \n",
       "51089         1986   1988                               NaN              NaN   \n",
       "51090         1986   1988                     Graney, Juris  None available.   \n",
       "51091         1986   1988                     Graney, Juris  None available.   \n",
       "51092         1986   1988                     Graney, Juris  None available.   \n",
       "51093         1986   1988                     Graney, Juris  None available.   \n",
       "\n",
       "                                                   Links  \\\n",
       "0      http://resolver.library.ualberta.ca/resolver?u...   \n",
       "1      http://resolver.library.ualberta.ca/resolver?u...   \n",
       "2      http://resolver.library.ualberta.ca/resolver?u...   \n",
       "3      http://resolver.library.ualberta.ca/resolver?u...   \n",
       "4      http://resolver.library.ualberta.ca/resolver?u...   \n",
       "...                                                  ...   \n",
       "51089                                                NaN   \n",
       "51090  http://resolver.library.ualberta.ca/resolver?u...   \n",
       "51091  http://resolver.library.ualberta.ca/resolver?u...   \n",
       "51092  http://resolver.library.ualberta.ca/resolver?u...   \n",
       "51093  http://resolver.library.ualberta.ca/resolver?u...   \n",
       "\n",
       "                                               Full Text  \\\n",
       "0      Gunnar Hkmark is the president of the Stockho...   \n",
       "1      Gunnar Hkmark is the president of the Stockho...   \n",
       "2      Gunnar Hkmark is the president of the Stockho...   \n",
       "3      Gunnar Hkmark is the president of the Stockho...   \n",
       "4      Gunnar Hkmark is the president of the Stockho...   \n",
       "...                                                  ...   \n",
       "51089                                                NaN   \n",
       "51090  Canada has deployed fresh troops from Edmonton...   \n",
       "51091  Canada has deployed fresh troops from Edmonton...   \n",
       "51092  Canada has deployed fresh troops from Edmonton...   \n",
       "51093  Canada has deployed fresh troops from Edmonton...   \n",
       "\n",
       "                                                   Title          Publication  \\\n",
       "0      How Sweden can collaborate with Canada: Ê The ...   The Globe and Mail   \n",
       "1      How Sweden can collaborate with Canada: Ê The ...   The Globe and Mail   \n",
       "2      How Sweden can collaborate with Canada: Ê The ...   The Globe and Mail   \n",
       "3      How Sweden can collaborate with Canada: Ê The ...   The Globe and Mail   \n",
       "4      How Sweden can collaborate with Canada: Ê The ...   The Globe and Mail   \n",
       "...                                                  ...                  ...   \n",
       "51089                                                NaN                  NaN   \n",
       "51090  Canadians sent on training mission; Edmonton-b...  The Simcoe Reformer   \n",
       "51091  Canadians sent on training mission; Edmonton-b...  The Simcoe Reformer   \n",
       "51092  Canadians sent on training mission; Edmonton-b...  The Simcoe Reformer   \n",
       "51093  Canadians sent on training mission; Edmonton-b...  The Simcoe Reformer   \n",
       "\n",
       "       ...  Publication Date        Section               Publisher  \\\n",
       "0      ...        2023-07-13           News      The Globe and Mail   \n",
       "1      ...        2023-07-13           News      The Globe and Mail   \n",
       "2      ...        2023-07-13           News      The Globe and Mail   \n",
       "3      ...        2023-07-13           News      The Globe and Mail   \n",
       "4      ...        2023-07-13           News      The Globe and Mail   \n",
       "...    ...               ...            ...                     ...   \n",
       "51089  ...               NaN            NaN                     NaN   \n",
       "51090  ...        2016-08-05  National News  Postmedia Network Inc.   \n",
       "51091  ...        2016-08-05  National News  Postmedia Network Inc.   \n",
       "51092  ...        2016-08-05  National News  Postmedia Network Inc.   \n",
       "51093  ...        2016-08-05  National News  Postmedia Network Inc.   \n",
       "\n",
       "      Place of publication                   Publication subject Source type  \\\n",
       "0            Toronto, Ont.  General Interest Periodicals--Canada   Newspaper   \n",
       "1            Toronto, Ont.  General Interest Periodicals--Canada   Newspaper   \n",
       "2            Toronto, Ont.  General Interest Periodicals--Canada   Newspaper   \n",
       "3            Toronto, Ont.  General Interest Periodicals--Canada   Newspaper   \n",
       "4            Toronto, Ont.  General Interest Periodicals--Canada   Newspaper   \n",
       "...                    ...                                   ...         ...   \n",
       "51089                  NaN                                   NaN         NaN   \n",
       "51090              Calgary  General Interest Periodicals--Canada   Newspaper   \n",
       "51091              Calgary  General Interest Periodicals--Canada   Newspaper   \n",
       "51092              Calgary  General Interest Periodicals--Canada   Newspaper   \n",
       "51093              Calgary  General Interest Periodicals--Canada   Newspaper   \n",
       "\n",
       "              Document type Publication Location  \\\n",
       "0      Opinions, Commentary        Toronto, Ont.   \n",
       "1      Opinions, Commentary        Toronto, Ont.   \n",
       "2      Opinions, Commentary        Toronto, Ont.   \n",
       "3      Opinions, Commentary        Toronto, Ont.   \n",
       "4      Opinions, Commentary        Toronto, Ont.   \n",
       "...                     ...                  ...   \n",
       "51089                   NaN                  NaN   \n",
       "51090                  News              Calgary   \n",
       "51091                  News              Calgary   \n",
       "51092                  News              Calgary   \n",
       "51093                  News              Calgary   \n",
       "\n",
       "                                              clean_text  \\\n",
       "0      gunnar hkmark president stockholm free world f...   \n",
       "1      patrik oksanen senior fellow stockholm free wo...   \n",
       "2              sweden canada share many values interests   \n",
       "3      two arctic countries mutual concerns imperiali...   \n",
       "4      sweden canada also suffered troubling forest f...   \n",
       "...                                                  ...   \n",
       "51089                                                NaN   \n",
       "51090  like modern army find interoperate partners al...   \n",
       "51091                                           sentence   \n",
       "51092                                            twitter   \n",
       "51093                                     comjurisgraney   \n",
       "\n",
       "      Global_Citizen_predictions  \n",
       "0                            0.0  \n",
       "1                            0.0  \n",
       "2                            0.0  \n",
       "3                            0.0  \n",
       "4                            0.0  \n",
       "...                          ...  \n",
       "51089                        NaN  \n",
       "51090                        0.0  \n",
       "51091                        0.0  \n",
       "51092                        0.0  \n",
       "51093                        0.0  \n",
       "\n",
       "[51094 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wandb\n",
      "  Downloading wandb-0.15.12-py3-none-any.whl (2.1 MB)\n",
      "     ---------------------------------------- 2.1/2.1 MB 3.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in c:\\users\\mitra\\appdata\\roaming\\python\\python39\\site-packages (from wandb) (7.1.2)\n",
      "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Downloading GitPython-3.1.37-py3-none-any.whl (190 kB)\n",
      "     -------------------------------------- 190.0/190.0 kB 5.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\mitra\\anaconda3\\lib\\site-packages (from wandb) (2.25.1)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\mitra\\anaconda3\\lib\\site-packages (from wandb) (5.9.0)\n",
      "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-1.31.0-py2.py3-none-any.whl (224 kB)\n",
      "     -------------------------------------- 224.8/224.8 kB 4.6 MB/s eta 0:00:00\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\mitra\\anaconda3\\lib\\site-packages (from wandb) (6.0)\n",
      "Collecting pathtools (from wandb)\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting setproctitle (from wandb)\n",
      "  Downloading setproctitle-1.3.3-cp39-cp39-win_amd64.whl (11 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\mitra\\anaconda3\\lib\\site-packages (from wandb) (63.4.1)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in c:\\users\\mitra\\anaconda3\\lib\\site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\mitra\\anaconda3\\lib\\site-packages (from wandb) (4.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in c:\\users\\mitra\\anaconda3\\lib\\site-packages (from wandb) (3.19.4)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\users\\mitra\\anaconda3\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\mitra\\anaconda3\\lib\\site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.9)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\mitra\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\mitra\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\mitra\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mitra\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2023.5.7)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\mitra\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n",
      "Building wheels for collected packages: pathtools\n",
      "  Building wheel for pathtools (setup.py): started\n",
      "  Building wheel for pathtools (setup.py): finished with status 'done'\n",
      "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8792 sha256=59a00707f9c326af08f1d0e40d9dc5ea1f2b4ce23a7fc827bef99d2036b4074b\n",
      "  Stored in directory: c:\\users\\mitra\\appdata\\local\\pip\\cache\\wheels\\b7\\0a\\67\\ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\n",
      "Successfully built pathtools\n",
      "Installing collected packages: pathtools, setproctitle, sentry-sdk, docker-pycreds, GitPython, wandb\n",
      "  Attempting uninstall: GitPython\n",
      "    Found existing installation: GitPython 3.1.29\n",
      "    Uninstalling GitPython-3.1.29:\n",
      "      Successfully uninstalled GitPython-3.1.29\n",
      "Successfully installed GitPython-3.1.37 docker-pycreds-0.4.0 pathtools-0.1.2 sentry-sdk-1.31.0 setproctitle-1.3.3 wandb-0.15.12\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mitra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_irrelevant(df):\n",
    "\n",
    "    \"\"\"\n",
    "    remove text smaller than 15 chars or the ones that are not a sentence\n",
    "    \"\"\"\n",
    "    mask = (df['clean_text'].str.len() > 15) & (df['clean_text'] != \"not a sentence\")\n",
    "    filtered_df = df[mask]\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "label = 'Global Citizen'\n",
    "drop_duplicates_for_train = True\n",
    "ratio = 0.2 # ratio of test to the whole dataset. \"same\" for a balanced dataset\n",
    "NUM_LABELS = 1\n",
    "LR = 2e-5\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 32\n",
    "NUM_FOLDS = 3 # cross val\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "PROJECT_SWEEP_NAME = label\n",
    "BASE_FOLDER = \"/home/mitrasadat.mirshafie/JC_folder/hailey/global citizen/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length is  2741\n",
      "length is  2741\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_concat = pd.read_csv(r\"C:\\Users\\mitra\\Downloads\\df_concat_Global Citizen.csv\")\n",
    "print('length is ', len(df_concat))\n",
    "df_concat = df_concat.dropna(subset=[label, \"clean_text\"])\n",
    "print('length is ', len(df_concat))\n",
    "df_concat.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_concat = remove_irrelevant(df_concat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zeros length =  2544\n",
      "ones length =  197\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b23e523cc68c42a4b7f56e06fe5c7b14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fb65416777e496cbb91a43c295c0022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "886bd6d48cf5477d9d86403f7f4e6e25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a770c0f2e7445f886eda2f0189644ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Tokenize and format the data\n",
    "def tokenize_data(texts, labels, max_length=128):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for text in texts:\n",
    "        encoding = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "        input_ids.append(encoding['input_ids'])\n",
    "        attention_masks.append(encoding['attention_mask'])\n",
    "\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    return input_ids, attention_masks, labels\n",
    "\n",
    "\n",
    "def balanced(training, label='Global Citizen', ratio=1):\n",
    "\n",
    "\n",
    "    ones = training[training[label] == 1]\n",
    "    zeros = training[training[label] == 0]\n",
    "\n",
    "    n = int(1/ratio * len(ones))\n",
    "    print(n)\n",
    "\n",
    "    zeros = training[training[label] == 0].sample(n=n, random_state=42)\n",
    "\n",
    "    print('length of zeros = ', len(zeros))\n",
    "    print('length of ones = ', len(ones))\n",
    "\n",
    "    # Concatenate the balanced samples\n",
    "    df = pd.concat([zeros, ones])\n",
    "\n",
    "    # Shuffle the dataset\n",
    "    df = shuffle(df, random_state=42)\n",
    "\n",
    "    # Reset the index\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ---- stats \n",
    "zeros = df_concat[df_concat[label] == 0]\n",
    "ones = df_concat[df_concat[label] == 1]\n",
    "\n",
    "print('zeros length = ', len(zeros))\n",
    "print('ones length = ', len(ones))\n",
    "\n",
    "import wandb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('cuda!')\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Apply cross-validation\n",
    "skf = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "wandb: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def train(config=None, checkpoint_dir=\"\"):\n",
    "\n",
    "\n",
    "    \n",
    "    df=balanced(df_concat, ratio=config['ratio'])\n",
    "\n",
    "    # Prepare your data------------------------------------------\n",
    "    X = df['clean_text'].values\n",
    "    y = df[label].values\n",
    "\n",
    "    f1_list = []\n",
    "    accuracy_list = []\n",
    "    recall_list = []\n",
    "    roc_auc_list = []\n",
    "\n",
    "    LR = config['learning_rate']\n",
    "    NUM_EPOCHS = config['num_epoch']\n",
    "    RATIO = config['ratio']\n",
    "    BATCH_SIZE = ['batch_size']\n",
    "\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(skf.split(X, y)):\n",
    "        print(f\"Fold {fold + 1}/{NUM_FOLDS}\")\n",
    "\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "        # model creation\n",
    "        model = BertForSequenceClassification.from_pretrained(MODEL_NAME,\n",
    "                                                              num_labels=NUM_LABELS)\n",
    "        model.to(device)\n",
    "\n",
    "        # Tokenize and format the training and validation data\n",
    "        input_ids_train, attention_masks_train, y_train = tokenize_data(X_train, y_train)\n",
    "        input_ids_val, attention_masks_val, y_val = tokenize_data(X_val, y_val)\n",
    "\n",
    "        input_ids_train = input_ids_train.to(device)\n",
    "        attention_masks_train = attention_masks_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "\n",
    "        input_ids_val = input_ids_val.to(device)\n",
    "        attention_masks_val = attention_masks_val.to(device)\n",
    "        y_val = y_val.to(device)\n",
    "\n",
    "        # Create DataLoader for training and validation data\n",
    "\n",
    "        train_data = TensorDataset(input_ids_train, attention_masks_train, y_train)\n",
    "        train_sampler = RandomSampler(train_data)\n",
    "        train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "        val_data = TensorDataset(input_ids_val, attention_masks_val, y_val)\n",
    "        val_dataloader = DataLoader(val_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "        # Define optimizer and loss function\n",
    "        optimizer = AdamW(model.parameters(), lr=LR)\n",
    "        loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "\n",
    "            for batch in train_dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                input_ids, attention_mask, labels = batch\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                labels = labels.unsqueeze(1).float()\n",
    "                loss = loss_fn(logits, labels)\n",
    "                total_loss += loss.item()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            avg_train_loss = total_loss / len(train_dataloader)\n",
    "            print(f'Epoch {epoch + 1}/{NUM_EPOCHS} - Average training loss: {avg_train_loss:.4f}')\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                input_ids, attention_mask, labels = batch\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                val_preds.extend(logits.sigmoid().round().squeeze(1).tolist())\n",
    "                val_labels.extend(labels.tolist())\n",
    "\n",
    "        # Calculate metrics for this fold\n",
    "        f1 = f1_score(val_labels, val_preds)\n",
    "        accuracy = accuracy_score(val_labels, val_preds)\n",
    "        recall = recall_score(val_labels, val_preds)\n",
    "        roc_auc = roc_auc_score(val_labels, val_preds)\n",
    "\n",
    "        f1_list.append(f1)\n",
    "        accuracy_list.append(accuracy)\n",
    "        recall_list.append(recall)\n",
    "        roc_auc_list.append(roc_auc)\n",
    "\n",
    "        print(f'Fold {fold + 1} Metrics:')\n",
    "        print(f'F1 Score: {f1:.4f}')\n",
    "        print(f'Accuracy: {accuracy:.4f}')\n",
    "        print(f'Recall: {recall:.4f}')\n",
    "        print(f'ROC AUC: {roc_auc:.4f}')\n",
    "\n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(val_labels, val_preds)\n",
    "\n",
    "        # Get True Positives (TP) for class 1\n",
    "        tp_class_1 = cm[1, 1]\n",
    "        # Get True Negatives (TN) for class 0\n",
    "        tn_class_0 = cm[0, 0]\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['0', '1'], yticklabels=['0', '1'])\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "        metrics = {'tp_class_1': tp_class_1, 'tn_class_0': tn_class_0,\n",
    "                   'roc_auc': roc_auc, 'recall': recall, 'f1': f1,\n",
    "                   'valid_accuracy':accuracy, 'training_loss':avg_train_loss,\n",
    "                   'ratio': RATIO\n",
    "                   #'valid_loss':validation_loss, 'training_acc':top1_acc_train, \n",
    "                   }\n",
    "\n",
    "        wandb.log(metrics)\n",
    "\n",
    "\n",
    "        print(\"Finished Training\")\n",
    "        #test_acc(model, device, config)\n",
    "\n",
    "\n",
    "\n",
    "# HYPERPARAMETER SPACE DEFINITION---------------------------------------------\n",
    "\n",
    "# Refer for distributions: https://docs.wandb.ai/guides/sweeps/define-sweep-configuration\n",
    "\n",
    "sweep_config = {\n",
    "\n",
    "    # define the search method\n",
    "    # one of \"grid\", \"random\" or \"bayes\"\n",
    "    'method': 'bayes',\n",
    "\n",
    "    # define the metric (useful for bayesian sweeps)\n",
    "    'metric': {\n",
    "        'name': 'accuracy',\n",
    "        'goal': 'maximize'\n",
    "    }\n",
    "}\n",
    "\n",
    "parameters = {\n",
    "    # defining constant parameters\n",
    "    # 'dataset': {'value': 'SetFit/sst2'},\n",
    "\n",
    "    # define different types of losses for contrastive learning\n",
    "    # these losses comes from sentence_transformers library\n",
    "\n",
    "    'ratio': {\n",
    "        'distribution': 'categorical',\n",
    "        'values': [0.1, 0.3, 0.6, 1]\n",
    "\n",
    "    },\n",
    "\n",
    "    'batch_size': {\n",
    "        # integers between 4 and 64\n",
    "        'distribution': 'categorical',\n",
    "        'values': [8, 16, 32, 64, 128, 256]\n",
    "    },\n",
    "    'num_epochs': {\n",
    "        'distribution': 'int_uniform',\n",
    "        'min': 1,\n",
    "        'max': 4\n",
    "    },\n",
    "\n",
    "    'learning_rate': {\n",
    "        'distribution': 'categorical',\n",
    "        'values': [1e-6, 5e-6, 1e-5, 5e-5, 1e-4, 1e-3],\n",
    "    }\n",
    "}\n",
    "\n",
    "# adding the hyperparameters to the parameters field in the sweep_config dictionary\n",
    "sweep_config['parameters'] = parameters\n",
    "sweep_config\n",
    "\n",
    "\n",
    "modelname = 'BERT'\n",
    "modelpath = 'saved_checkpoint_' + modelname\n",
    "\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=PROJECT_SWEEP_NAME)\n",
    "\n",
    "\n",
    "wandb.agent(sweep_id, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "def remove_irrelevant(df):\n",
    "\n",
    "    \"\"\"\n",
    "    remove text smaller than 15 chars or the ones that are not a sentence\n",
    "    \"\"\"\n",
    "    mask = (df['clean_text'].str.len() > 15) & (df['clean_text'] != \"not a sentence\")\n",
    "    filtered_df = df[mask]\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "label = 'Global Citizen'\n",
    "drop_duplicates_for_train = True\n",
    "ratio = 0.2 # ratio of test to the whole dataset. \"same\" for a balanced dataset\n",
    "NUM_LABELS = 1\n",
    "LR = 2e-5\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 32\n",
    "NUM_FOLDS = 3 # cross val\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "PROJECT_SWEEP_NAME = label\n",
    "BASE_FOLDER = \"/home/mitrasadat.mirshafie/JC_folder/hailey/global citizen/\"\n",
    "\n",
    "df_concat = pd.read_csv(BASE_FOLDER + \"df_concat_Global Citizen.csv\")\n",
    "print('length is ', len(df_concat))\n",
    "df_concat = df_concat.dropna(subset=[label, \"clean_text\"])\n",
    "print('length is ', len(df_concat))\n",
    "df_concat.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_concat = remove_irrelevant(df_concat)\n",
    "\n",
    "\n",
    "# Tokenize and format the data\n",
    "def tokenize_data(texts, labels, max_length=128):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for text in texts:\n",
    "        encoding = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "        input_ids.append(encoding['input_ids'])\n",
    "        attention_masks.append(encoding['attention_mask'])\n",
    "\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    return input_ids, attention_masks, labels\n",
    "\n",
    "\n",
    "def balanced(training, label='Global Citizen', ratio=1):\n",
    "\n",
    "\n",
    "    ones = training[training[label] == 1]\n",
    "    zeros = training[training[label] == 0]\n",
    "\n",
    "    n = int(1/ratio * len(ones))\n",
    "    print(n)\n",
    "\n",
    "    zeros = training[training[label] == 0].sample(n=n, random_state=42)\n",
    "\n",
    "    print('length of zeros = ', len(zeros))\n",
    "    print('length of ones = ', len(ones))\n",
    "\n",
    "    # Concatenate the balanced samples\n",
    "    df = pd.concat([zeros, ones])\n",
    "\n",
    "    # Shuffle the dataset\n",
    "    df = shuffle(df, random_state=42)\n",
    "\n",
    "    # Reset the index\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ---- stats \n",
    "zeros = df_concat[df_concat[label] == 0]\n",
    "ones = df_concat[df_concat[label] == 1]\n",
    "\n",
    "print('zeros length = ', len(zeros))\n",
    "print('ones length = ', len(ones))\n",
    "\n",
    "import wandb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('cuda!')\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Apply cross-validation\n",
    "skf = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "def train(config=None, checkpoint_dir=\"\"):\n",
    "\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "      \n",
    "      df=balanced(df_concat, ratio=config['ratio'])\n",
    "  \n",
    "      # Prepare your data------------------------------------------\n",
    "      X = df['clean_text'].values\n",
    "      y = df[label].values\n",
    "  \n",
    "      f1_list = []\n",
    "      accuracy_list = []\n",
    "      recall_list = []\n",
    "      roc_auc_list = []\n",
    "  \n",
    "      LR = config['learning_rate']\n",
    "      NUM_EPOCHS = config['num_epoch']\n",
    "      RATIO = config['ratio']\n",
    "      BATCH_SIZE = ['batch_size']\n",
    "  \n",
    "  \n",
    "      for fold, (train_index, val_index) in enumerate(skf.split(X, y)):\n",
    "          print(f\"Fold {fold + 1}/{NUM_FOLDS}\")\n",
    "  \n",
    "          X_train, X_val = X[train_index], X[val_index]\n",
    "          y_train, y_val = y[train_index], y[val_index]\n",
    "  \n",
    "          # model creation\n",
    "          model = BertForSequenceClassification.from_pretrained(MODEL_NAME,\n",
    "                                                                num_labels=NUM_LABELS)\n",
    "          model.to(device)\n",
    "  \n",
    "          # Tokenize and format the training and validation data\n",
    "          print('Tokenize and format the training and validation data')\n",
    "          input_ids_train, attention_masks_train, y_train = tokenize_data(X_train, y_train)\n",
    "          input_ids_val, attention_masks_val, y_val = tokenize_data(X_val, y_val)\n",
    "  \n",
    "          input_ids_train = input_ids_train.to(device)\n",
    "          attention_masks_train = attention_masks_train.to(device)\n",
    "          y_train = y_train.to(device)\n",
    "  \n",
    "          input_ids_val = input_ids_val.to(device)\n",
    "          attention_masks_val = attention_masks_val.to(device)\n",
    "          y_val = y_val.to(device)\n",
    "  \n",
    "          # Create DataLoader for training and validation data\n",
    "          print('Create DataLoader for training and validation data')\n",
    "          train_data = TensorDataset(input_ids_train, attention_masks_train, y_train)\n",
    "          train_sampler = RandomSampler(train_data)\n",
    "          train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "  \n",
    "          val_data = TensorDataset(input_ids_val, attention_masks_val, y_val)\n",
    "          val_dataloader = DataLoader(val_data, batch_size=BATCH_SIZE)\n",
    "  \n",
    "          # Define optimizer and loss function\n",
    "          optimizer = AdamW(model.parameters(), lr=LR)\n",
    "          loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "  \n",
    "  \n",
    "          # Training loop\n",
    "          for epoch in range(NUM_EPOCHS):\n",
    "              model.train()\n",
    "              total_loss = 0\n",
    "  \n",
    "              for batch in train_dataloader:\n",
    "                  optimizer.zero_grad()\n",
    "                  input_ids, attention_mask, labels = batch\n",
    "                  outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                  logits = outputs.logits\n",
    "                  labels = labels.unsqueeze(1).float()\n",
    "                  loss = loss_fn(logits, labels)\n",
    "                  total_loss += loss.item()\n",
    "                  loss.backward()\n",
    "                  optimizer.step()\n",
    "  \n",
    "              avg_train_loss = total_loss / len(train_dataloader)\n",
    "              print(f'Epoch {epoch + 1}/{NUM_EPOCHS} - Average training loss: {avg_train_loss:.4f}')\n",
    "  \n",
    "          # Evaluation\n",
    "          model.eval()\n",
    "          val_preds = []\n",
    "          val_labels = []\n",
    "  \n",
    "          with torch.no_grad():\n",
    "              for batch in val_dataloader:\n",
    "                  input_ids, attention_mask, labels = batch\n",
    "                  outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                  logits = outputs.logits\n",
    "                  val_preds.extend(logits.sigmoid().round().squeeze(1).tolist())\n",
    "                  val_labels.extend(labels.tolist())\n",
    "  \n",
    "          # Calculate metrics for this fold\n",
    "          f1 = f1_score(val_labels, val_preds)\n",
    "          accuracy = accuracy_score(val_labels, val_preds)\n",
    "          recall = recall_score(val_labels, val_preds)\n",
    "          roc_auc = roc_auc_score(val_labels, val_preds)\n",
    "  \n",
    "          f1_list.append(f1)\n",
    "          accuracy_list.append(accuracy)\n",
    "          recall_list.append(recall)\n",
    "          roc_auc_list.append(roc_auc)\n",
    "  \n",
    "          print(f'Fold {fold + 1} Metrics:')\n",
    "          print(f'F1 Score: {f1:.4f}')\n",
    "          print(f'Accuracy: {accuracy:.4f}')\n",
    "          print(f'Recall: {recall:.4f}')\n",
    "          print(f'ROC AUC: {roc_auc:.4f}')\n",
    "  \n",
    "          # Confusion matrix\n",
    "          cm = confusion_matrix(val_labels, val_preds)\n",
    "  \n",
    "          # Get True Positives (TP) for class 1\n",
    "          tp_class_1 = cm[1, 1]\n",
    "          # Get True Negatives (TN) for class 0\n",
    "          tn_class_0 = cm[0, 0]\n",
    "  \n",
    "          plt.figure(figsize=(8, 6))\n",
    "          sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['0', '1'], yticklabels=['0', '1'])\n",
    "          plt.xlabel('Predicted')\n",
    "          plt.ylabel('True')\n",
    "          plt.title('Confusion Matrix')\n",
    "          plt.show()\n",
    "  \n",
    "  \n",
    "  \n",
    "          metrics = {'tp_class_1': tp_class_1, 'tn_class_0': tn_class_0,\n",
    "                     'roc_auc': roc_auc, 'recall': recall, 'f1': f1,\n",
    "                     'valid_accuracy':accuracy, 'training_loss':avg_train_loss,\n",
    "                     'ratio': RATIO\n",
    "                     #'valid_loss':validation_loss, 'training_acc':top1_acc_train, \n",
    "                     }\n",
    "  \n",
    "          wandb.log(metrics)\n",
    "  \n",
    "  \n",
    "          print(\"Finished Training\")\n",
    "          #test_acc(model, device, config)\n",
    "  \n",
    "\n",
    "\n",
    "# HYPERPARAMETER SPACE DEFINITION---------------------------------------------\n",
    "\n",
    "# Refer for distributions: https://docs.wandb.ai/guides/sweeps/define-sweep-configuration\n",
    "\n",
    "sweep_config = {\n",
    "\n",
    "    # define the search method\n",
    "    # one of \"grid\", \"random\" or \"bayes\"\n",
    "    'method': 'bayes',\n",
    "\n",
    "    # define the metric (useful for bayesian sweeps)\n",
    "    'metric': {\n",
    "        'name': 'accuracy',\n",
    "        'goal': 'maximize'\n",
    "    }\n",
    "}\n",
    "\n",
    "parameters = {\n",
    "    # defining constant parameters\n",
    "    # 'dataset': {'value': 'SetFit/sst2'},\n",
    "\n",
    "    # define different types of losses for contrastive learning\n",
    "    # these losses comes from sentence_transformers library\n",
    "\n",
    "    'ratio': {\n",
    "        'distribution': 'categorical',\n",
    "        'values': [0.1, 0.3, 0.6, 1]\n",
    "\n",
    "    },\n",
    "\n",
    "    'batch_size': {\n",
    "        # integers between 4 and 64\n",
    "        'distribution': 'categorical',\n",
    "        'values': [8, 16, 32, 64, 128, 256]\n",
    "    },\n",
    "    'num_epochs': {\n",
    "        'distribution': 'int_uniform',\n",
    "        'min': 1,\n",
    "        'max': 4\n",
    "    },\n",
    "\n",
    "    'learning_rate': {\n",
    "        'distribution': 'categorical',\n",
    "        'values': [1e-6, 5e-6, 1e-5, 5e-5, 1e-4, 1e-3],\n",
    "    }\n",
    "}\n",
    "\n",
    "# adding the hyperparameters to the parameters field in the sweep_config dictionary\n",
    "sweep_config['parameters'] = parameters\n",
    "sweep_config\n",
    "\n",
    "\n",
    "modelname = 'BERT'\n",
    "modelpath = 'saved_checkpoint_' + modelname\n",
    "\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=PROJECT_SWEEP_NAME)\n",
    "\n",
    "\n",
    "wandb.agent(sweep_id, train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
